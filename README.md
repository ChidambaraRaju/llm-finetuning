# My LLM Fine-Tuning Journey ðŸš€

Welcome to my personal learning repository! This space is dedicated to documenting my journey as I dive into the world of Large Language Model (LLM) Fine-Tuning. The notes, code, and experiments here serve as my digital notebook, helping me solidify concepts and track my progress.

The content is primarily based on my studies from online courses and playlists. My goal is to create a valuable reference for my future self and to showcase the hands-on process of learning this exciting technology.

> **Note:** This repository is a living document. It will be updated continuously as I progress through new topics and complete more hands-on exercises.

---

## My Learning Log ðŸ“š

Here you'll find a structured collection of my notes from each lecture or topic I cover.

* **Lecture 01: Introduction to Fine-Tuning**
    * **File:** [`lecture-01-introduction-to-finetuning.md`](./lecture-01-introduction-to-finetuning.md)
    * **Topics Covered:** The Transformer architecture, BERT vs. GPT, and the 3 major stages of LLM training (Pre-training, SFT, Alignment).

* **Lecture 02: What is LLM Fine-Tuning?**
    * **File:** [`lecture-02-what-is-llm-finetuning.md`](./lecture-02-what-is-llm-finetuning.md)
    * **Topics Covered:** Transfer Learning vs. Fine-Tuning, use cases, advantages/disadvantages, and an overview of popular fine-tuning frameworks.

* *More notes and code to come...*

---

## How to Use This Repo

Feel free to browse the notes and code. If you're on a similar learning path, I hope my notes can offer a helpful perspective. For any code examples, please refer to the specific folders for instructions on setup and dependencies.