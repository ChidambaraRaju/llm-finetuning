{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-cell"
      },
      "source": [
        "#  Evaluating a Base LLM for Summarization (Baseline Tutorial)\n",
        "\n",
        "This notebook walks through the process of establishing a *baseline* performance for a large language model (LLM) on a summarization task.\n",
        "\n",
        "**Goal:** To understand how the original, un-finetuned model performs so we have a benchmark to compare against after fine-tuning.\n",
        "\n",
        "**Task:** Summarizing the [SAMSum Dataset](https://huggingface.co/datasets/knkarthick/samsum).\n",
        "\n",
        "**Metric:** We will use the ROUGE score to evaluate the quality of the generated summaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-1"
      },
      "source": [
        "## 1. ðŸ“š Setup: Install Dependencies\n",
        "\n",
        "First, we need to install all the required Python libraries.\n",
        "\n",
        "* `evaluate`: Hugging Face's library for model evaluation (used for ROUGE).\n",
        "* `torch`: The core PyTorch library.\n",
        "* `datasets`: For loading the SAMSum dataset.\n",
        "* `peft`: Parameter-Efficient Fine-Tuning (used for LoRA).\n",
        "* `transformers`: For loading the model and tokenizer.\n",
        "* `bitsandbytes`: For quantization (which we will disable for this baseline but is needed for the helper functions).\n",
        "* `rouge_score`: The specific package needed by `evaluate` for ROUGE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsvjbjgYWuTX",
        "outputId": "c0e85a5d-8103-4dd5-95ff-bd642fe29143"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for Hugging Face, evaluation, and PEFT\n",
        "! pip install -q evaluate torch tqdm datasets peft transformers rouge_score\n",
        "\n",
        "# Install bitsandbytes for quantization support\n",
        "! pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-2"
      },
      "source": [
        "## 2. ðŸ“¦ Import Libraries\n",
        "\n",
        "Now, let's import all the modules we'll use throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JVUpl_sW316"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml  # For loading the configuration file\n",
        "import json  # For saving results\n",
        "import torch\n",
        "import evaluate  # For loading the ROUGE metric\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning) libraries\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Transformers libraries for model, tokenizer, and quantization\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmwYiv9bXEiH"
      },
      "outputs": [],
      "source": [
        "# Define a constant for the dataset directory (if loading from disk)\n",
        "DATASETS_DIR = \"./datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-3"
      },
      "source": [
        "## 3. ðŸ’¾ Data Loading and Preparation\n",
        "\n",
        "We'll define two helper functions to manage our dataset.\n",
        "\n",
        "* `select_subset`: A utility to select a specific number of samples from a dataset split (e.g., \"all\" or 200). This is useful for running quick experiments.\n",
        "* `load_and_prepare_dataset`: The main function that loads the dataset from Hugging Face (based on the config) and uses `select_subset` to prepare our `train`, `val`, and `test` splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxBrMbBPXRxe"
      },
      "outputs": [],
      "source": [
        "def select_subset(dataset, n_samples, seed=42):\n",
        "    \"\"\"\n",
        "    Select a subset of the dataset.\n",
        "    If n_samples is \"all\" or None, return the entire dataset.\n",
        "    Otherwise, sample n_samples examples.\n",
        "    \"\"\"\n",
        "    if n_samples == \"all\" or n_samples is None:\n",
        "        return dataset\n",
        "\n",
        "    if n_samples > len(dataset):\n",
        "        print(f\"âš ï¸  Requested {n_samples} samples but only {len(dataset)} available. Using all samples.\")\n",
        "        return dataset\n",
        "\n",
        "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
        "\n",
        "\n",
        "def load_and_prepare_dataset(cfg):\n",
        "    \"\"\"\n",
        "    Load dataset splits according to configuration.\n",
        "    \"\"\"\n",
        "    cfg_dataset = cfg[\"dataset\"]\n",
        "    dataset = load_dataset(cfg_dataset[\"name\"])\n",
        "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
        "\n",
        "    train = select_subset(dataset[\"train\"], cfg_dataset[\"splits\"][\"train\"], seed=42)\n",
        "    val = select_subset(dataset[val_key], cfg_dataset[\"splits\"][\"validation\"], seed=42)\n",
        "    test = select_subset(dataset[\"test\"], cfg_dataset[\"splits\"][\"test\"], seed=42)\n",
        "\n",
        "    print(f\"ðŸ“Š Loaded {len(train)} train / {len(val)} val / {len(test)} test samples.\")\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-4"
      },
      "source": [
        "## 4. ðŸ¤– Model & Tokenizer Setup\n",
        "\n",
        "This is a crucial helper function responsible for loading the model and tokenizer. It's designed to be flexible and can handle three main scenarios:\n",
        "\n",
        "1.  **Full Precision (Baseline):** Loads the model as-is (e.g., in `bfloat16`). This is what we'll use for our baseline.\n",
        "2.  **Quantized (QLoRA Prep):** Loads the model in 4-bit precision using `BitsAndBytesConfig`. This is the 'Q' in QLoRA, a technique to reduce memory usage.\n",
        "3.  **Quantized + LoRA:** Applies LoRA (Low-Rank Adaptation) adapters to the quantized model, making it ready for PEFT (Parameter-Efficient Fine-Tuning).\n",
        "\n",
        "For this tutorial, we will force it into **scenario 1** by passing `use_4bit=False` and `use_lora=False` in the final step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jh1JqIzZFkR"
      },
      "outputs": [],
      "source": [
        "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
        "    \"\"\"\n",
        "    Load model, tokenizer, and apply quantization + LoRA config if specified.\n",
        "\n",
        "    Args:\n",
        "        cfg (dict): Configuration dictionary containing:\n",
        "            - base_model\n",
        "            - quantization parameters\n",
        "            - lora parameters (optional)\n",
        "            - bf16 or fp16 precision\n",
        "        use_4bit (bool, optional): Override whether to load in 4-bit mode.\n",
        "        use_lora (bool, optional): Override whether to apply LoRA adapters.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer)\n",
        "    \"\"\"\n",
        "    model_name = cfg[\"base_model\"]\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Tokenizer setup\n",
        "    # ------------------------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    # Determine quantization + LoRA usage\n",
        "    # Use override if provided, otherwise default to config file\n",
        "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
        "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Quantization setup\n",
        "    # ------------------------------\n",
        "    quant_cfg = None\n",
        "    if load_in_4bit:\n",
        "        print(\"âš™ï¸  Enabling 4-bit quantization (QLoRA)...\")\n",
        "        quant_cfg = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"),\n",
        "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
        "            bnb_4bit_compute_dtype=getattr(\n",
        "                torch, cfg.get(\"bnb_4bit_compute_dtype\", \"bfloat16\")\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        print(\"âš™ï¸  Loading model in full precision (no quantization).\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Model loading\n",
        "    # ------------------------------\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_cfg, # Will be None if not 4-bit\n",
        "        device_map=\"auto\", # Automatically spread model across GPUs\n",
        "        dtype=(\n",
        "            torch.bfloat16\n",
        "            if cfg.get(\"bf16\", True) and torch.cuda.is_available()\n",
        "            else torch.float32\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ------------------------------\n",
        "    # LoRA setup (optional)\n",
        "    # ------------------------------\n",
        "    if apply_lora:\n",
        "        print(\"ðŸ”§ Applying LoRA configuration...\")\n",
        "        # Prepare model for k-bit training if quantization is enabled\n",
        "        if load_in_4bit:\n",
        "            model = prepare_model_for_kbit_training(model)\n",
        "        \n",
        "        lora_cfg = LoraConfig(\n",
        "            r=cfg.get(\"lora_r\", 8),\n",
        "            lora_alpha=cfg.get(\"lora_alpha\", 16),\n",
        "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]),\n",
        "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "        model.print_trainable_parameters()\n",
        "    else:\n",
        "        print(\"ðŸ”¹ Skipping LoRA setup â€” using base model only.\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-5-config"
      },
      "source": [
        "## 5. âš™ï¸ Configuration\n",
        "\n",
        "We use a central `config.yaml` file to manage all our settings, such as the model name, dataset, and LoRA parameters. This makes our experiments reproducible and easy to modify.\n",
        "\n",
        "### â— Action Required: Create `config.yaml`\n",
        "\n",
        "You must create a file named `config.yaml` in the **same directory** as this notebook. Copy and paste the following content into it.\n",
        "\n",
        "This configuration is based on the file you provided and sets up the model (`meta-llama/Llama-3.2-1B-Instruct`), dataset (`knkarthick/samsum`), and various training parameters (which we won't use in *this* notebook, but are needed for the file to load).\n",
        "\n",
        "```yaml\n",
        "# ==============================================\n",
        "# Model Configuration\n",
        "# ==============================================\n",
        "base_model: meta-llama/Llama-3.2-1B-Instruct\n",
        "tokenizer_type: meta-llama/Llama-3.2-1B-Instruct\n",
        "\n",
        "# ==============================================\n",
        "# Dataset\n",
        "# ==============================================\n",
        "dataset:\n",
        "  name: knkarthick/samsum\n",
        "  cache_dir: ../data/datasets\n",
        "  field_map:\n",
        "    input: dialogue\n",
        "    output: summary\n",
        "  type: completion\n",
        "  splits:\n",
        "    train: all\n",
        "    validation: 200\n",
        "    test: 200\n",
        "  seed: 42\n",
        "\n",
        "# This is for Axolotl\n",
        "datasets:\n",
        "  - path: knkarthick/samsum\n",
        "    cache_dir: ../data/datasets\n",
        "    field_map:\n",
        "      input: dialogue\n",
        "      output: summary\n",
        "    type: completion\n",
        "\n",
        "task_instruction: >\n",
        "  You are a helpful assistant who writes concise, factual summaries of conversations.\n",
        "  Summarize the following conversation into a single sentence.\n",
        "\n",
        "train_samples: all\n",
        "val_samples: 200\n",
        "test_samples: 200\n",
        "seed: 42\n",
        "\n",
        "# ==============================================\n",
        "# Quantization (for QLoRA)\n",
        "# ==============================================\n",
        "load_in_4bit: true\n",
        "bnb_4bit_quant_type: nf4\n",
        "bnb_4bit_use_double_quant: true\n",
        "bnb_4bit_compute_dtype: bfloat16\n",
        "\n",
        "# ==============================================\n",
        "# LoRA Configuration\n",
        "# ==============================================\n",
        "lora_r: 16\n",
        "lora_alpha: 32\n",
        "lora_dropout: 0.1\n",
        "target_modules: [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "# ==============================================\n",
        "# Training Configuration\n",
        "# ==============================================\n",
        "num_epochs: 1\n",
        "max_steps: 300\n",
        "learning_rate: 2e-4\n",
        "batch_size: 4\n",
        "gradient_accumulation_steps: 4\n",
        "sequence_len: 512\n",
        "lr_scheduler: cosine\n",
        "warmup_steps: 50\n",
        "bf16: true\n",
        "logging_steps: 25\n",
        "save_steps: 100\n",
        "save_total_limit: 2\n",
        "optim: paged_adamw_8bit\n",
        "\n",
        "# ==============================================\n",
        "# Output & Logging\n",
        "# ==============================================\n",
        "output_dir: ./outputs/lora_samsum\n",
        "wandb_project: llama3_samsum\n",
        "wandb_run_name: lora-finetuning-default-hps\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSRSJQOnaV1Q"
      },
      "outputs": [],
      "source": [
        "# Define the path to our configuration file\n",
        "CONFIG_FILE_PATH = \"./config.yaml\"\n",
        "\n",
        "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
        "    \"\"\"\n",
        "    Load and parse a YAML configuration file.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the config file.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed configuration dictionary.\n",
        "    \"\"\"\n",
        "    # Add a check to ensure the file exists\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Configuration file not found at: {config_path}. \"\n",
        "            \"Please create it using the template in the Markdown cell above.\"\n",
        "        )\n",
        "        \n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-6-eval"
      },
      "source": [
        "## 6. ðŸ“ˆ Evaluation Functions\n",
        "\n",
        "Here we define the core logic for our evaluation.\n",
        "\n",
        "### `generate_predictions`\n",
        "\n",
        "This function takes the model, tokenizer, and dataset, then:\n",
        "1.  Formats each 'dialogue' into a chat prompt using the `task_instruction` from our config.\n",
        "2.  Uses a Hugging Face `pipeline` for efficient, batched text generation.\n",
        "3.  Collects all the generated 'summaries' (the predictions).\n",
        "\n",
        "### `compute_rouge`\n",
        "\n",
        "This function calculates the ROUGE score, a standard metric for summarization.\n",
        "\n",
        "**What is ROUGE?** ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap between the model-generated summary and a human-written 'reference' summary.\n",
        "* **ROUGE-1:** Measures overlap of unigrams (single words).\n",
        "* **ROUGE-2:** Measures overlap of bigrams (pairs of words).\n",
        "* **ROUGE-L:** Measures the longest common subsequence (LCS), which checks for sentence-level structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS2bHGiVadF8"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    task_instruction,\n",
        "    num_samples=None,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate model predictions for a dataset (e.g., summaries).\n",
        "\n",
        "    Args:\n",
        "        model: The loaded model (base or fine-tuned).\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        dataset: Hugging Face dataset split containing 'dialogue' and 'summary'.\n",
        "        task_instruction (str): Instruction prefix for generation.\n",
        "        num_samples (int, optional): Number of samples to evaluate.\n",
        "        batch_size (int): Number of examples per inference batch.\n",
        "        max_new_tokens (int): Max tokens to generate per sample.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Generated summaries.\n",
        "    \"\"\"\n",
        "    # Select a subset of the dataset if num_samples is specified\n",
        "    if num_samples is not None and num_samples < len(dataset):\n",
        "        dataset = dataset.select(range(num_samples))\n",
        "\n",
        "    # Prepare prompts\n",
        "    prompts = []\n",
        "    for sample in dataset:\n",
        "        # Format the input using the instruction and dialogue\n",
        "        user_prompt = (\n",
        "            f\"{task_instruction}\\n\\n\"\n",
        "            f\"## Dialogue:\\n{sample['dialogue']}\\n\"\n",
        "            \"## Summary:\"\n",
        "        )\n",
        "        # Use the chat template for the specific model (e.g., Llama 3's)\n",
        "        messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Initialize the text-generation pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dtype=\"auto\", # Use model's native dtype (e.g., bfloat16)\n",
        "        do_sample=False, # Use greedy decoding for deterministic output\n",
        "    )\n",
        "\n",
        "    # Generate predictions in batches for efficiency\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating summaries\"):\n",
        "        batch = prompts[i : i + batch_size]\n",
        "        # 'return_full_text=False' ensures we only get the generated summary,\n",
        "        # not the prompt + summary\n",
        "        outputs = pipe(batch, max_new_tokens=max_new_tokens, return_full_text=False)\n",
        "        preds.extend([o[0][\"generated_text\"].strip() for o in outputs])\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_rouge(predictions, samples):\n",
        "    \"\"\"\n",
        "    Compute ROUGE scores between predictions and reference summaries.\n",
        "\n",
        "    Args:\n",
        "        predictions (list[str]): Model-generated outputs.\n",
        "        samples (datasets.Dataset): Dataset containing reference 'summary' field.\n",
        "\n",
        "    Returns:\n",
        "        dict: ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "    \"\"\"\n",
        "    # Load the ROUGE metric from the 'evaluate' library\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    # Get the ground-truth summaries from the dataset\n",
        "    references = [s[\"summary\"] for s in samples]\n",
        "    # Compute the scores\n",
        "    return rouge.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-7-run"
      },
      "source": [
        "## 7. ðŸš€ Run Baseline Evaluation\n",
        "\n",
        "It's time to put everything together!\n",
        "\n",
        "Here's the plan:\n",
        "1.  **Load Config:** Read the `config.yaml` file.\n",
        "2.  **Load Data:** Load the validation split of the SAMSum dataset.\n",
        "3.  **Load Model:** Call our `setup_model_and_tokenizer` function.\n",
        "    * **Crucially**, we will pass `use_4bit=False` and `use_lora=False`. This tells the function to ignore the quantization and LoRA settings in the config file and load the **true, original base model** in its standard precision (e.g., bfloat16). This is our baseline.\n",
        "4.  **Generate:** Run the model over the validation set to get predictions.\n",
        "5.  **Score:** Compute the ROUGE scores for the predictions.\n",
        "6.  **Save & Print:** Save the results to JSON files and print a summary to the screen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "RXEUxu8pb_co",
        "outputId": "2575343c-3709-48d1-8dec-defc2d293250"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "evaluate_baseline.py\n",
        "Evaluate the base (unfine-tuned) model on the SAMSum dataset to establish baseline ROUGE scores.\n",
        "\"\"\"\n",
        "\n",
        "# Load the global configuration from config.yaml\n",
        "cfg = load_config()\n",
        "\n",
        "def evaluate_baseline():\n",
        "    \"\"\"Run baseline evaluation on the SAMSum dataset using the base model.\"\"\"\n",
        "\n",
        "    # 1. Load validation data\n",
        "    # We only need the validation split for this evaluation\n",
        "    _, val_data, _ = load_and_prepare_dataset(cfg)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # 2. Load model + tokenizer (IMPORTANT: Baseline Configuration)\n",
        "    # ----------------------------------------------------------------------\n",
        "    print(\"\\nLoading the TRUE baseline model (full precision, no LoRA)...\")\n",
        "    model, tokenizer = setup_model_and_tokenizer(\n",
        "        cfg=cfg,\n",
        "        use_4bit=False,  # <-- Set to False to load the original model\n",
        "        use_lora=False,  # <-- Set to False to skip adding LoRA adapters\n",
        "    )\n",
        "\n",
        "    # 3. Generate predictions\n",
        "    preds = generate_predictions(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=val_data,\n",
        "        task_instruction=cfg[\"task_instruction\"],\n",
        "        batch_size=4, # Adjust batch size based on your GPU memory\n",
        "    )\n",
        "\n",
        "    # 4. Compute ROUGE metrics\n",
        "    scores = compute_rouge(preds, val_data)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # 5. Save outputs\n",
        "    # ----------------------------------------------------------------------\n",
        "    results = {\n",
        "        \"model_name\": cfg[\"base_model\"] + \" (Baseline)\",\n",
        "        \"num_samples\": len(val_data),\n",
        "        \"rouge1\": scores[\"rouge1\"],\n",
        "        \"rouge2\": scores[\"rouge2\"],\n",
        "        \"rougeL\": scores[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "    # Define output file paths\n",
        "    results_path = \"eval_results_baseline.json\"\n",
        "    preds_path = \"predictions_baseline.jsonl\"\n",
        "\n",
        "    # Save the summary scores\n",
        "    with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Save the detailed predictions (dialogue, reference, prediction)\n",
        "    with open(preds_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, pred in enumerate(preds):\n",
        "            json.dump(\n",
        "                {\n",
        "                    \"dialogue\": val_data[i][\"dialogue\"],\n",
        "                    \"reference\": val_data[i][\"summary\"],\n",
        "                    \"prediction\": pred,\n",
        "                },\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Saved results to: {results_path}\")\n",
        "    print(f\"ðŸ’¾ Saved predictions to: {preds_path}\")\n",
        "\n",
        "    # Return all components needed for the final report\n",
        "    return scores, preds, val_data\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Main Execution Block\n",
        "# --------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸš€ Starting baseline evaluation...\")\n",
        "    \n",
        "    # Run the evaluation\n",
        "    rouge_scores, predictions, val_samples = evaluate_baseline()\n",
        "    \n",
        "    print(\"\\nâœ… Evaluation complete.\")\n",
        "\n",
        "\n",
        "    print(\"\\nðŸ“ˆ Baseline ROUGE Results:\")\n",
        "    # Print scores formatted as percentages\n",
        "    print(f\"  ROUGE-1: {rouge_scores['rouge1'] * 100:.2f}%\")\n",
        "    print(f\"  ROUGE-2: {rouge_scores['rouge2'] * 100:.2f}%\")\n",
        "    print(f\"  ROUGE-L: {rouge_scores['rougeL'] * 100:.2f}%\")\n",
        "\n",
        "    # Show an example of the model's output vs. the reference\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ” Example Prediction (Sample 0)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\\n[DIALOGUE]\\n{val_samples[0]['dialogue']}\\n\")\n",
        "    print(f\"[REFERENCE SUMMARY]\\n{val_samples[0]['summary']}\\n\")\n",
        "    print(f\"[MODEL PREDICTION]\\n{predictions[0]}\")\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-8-conclusion"
      },
      "source": [
        "## 8. ðŸ Conclusion & Next Steps\n",
        "\n",
        "Congratulations! You've successfully established a baseline ROUGE score for your model.\n",
        "\n",
        "This score is your 'line in the sand.' It represents the model's out-of-the-box performance on this task.\n",
        "\n",
        "### Why does this matter?\n",
        "\n",
        "When you proceed to fine-tune this model (for example, using the QLoRA settings in your `config.yaml`), you can run this same evaluation script on your *new*, *fine-tuned* model.\n",
        "\n",
        "By comparing the new ROUGE scores to this baseline, you can definitively measure exactly how much your fine-tuning improved the model's summarization ability."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
