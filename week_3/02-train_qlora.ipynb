{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Tutorial: Fine-Tuning Llama 3.2 with QLoRA\n",
    "\n",
    "Welcome to this hands-on tutorial on fine-tuning Large Language Models (LLMs)! \n",
    "\n",
    "In this notebook, we will fine-tune the **Llama 3.2** model on the **SAMSum dataset** (dialogue summarization) using a technique called **QLoRA** (Quantized Low-Rank Adaptation).\n",
    "\n",
    "### üöÄ What you will learn:\n",
    "1.  **QLoRA:** How to fit a large model into memory using 4-bit quantization (NF4) and Double Quantization.\n",
    "2.  **LoRA Adapters:** How to train only a tiny fraction (<1%) of the parameters to save time and compute.\n",
    "3.  **Assistant-Only Masking:** A critical data preprocessing technique to ensure the model only learns to generate *responses*, not *prompts*.\n",
    "4.  **Hugging Face TRL/PEFT:** How to use the modern stack for efficient training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we need to install the necessary libraries. \n",
    "* `peft`: For Parameter-Efficient Fine-Tuning (LoRA).\n",
    "* `bitsandbytes`: For 4-bit quantization.\n",
    "* `transformers`: The core library for loading models.\n",
    "* `evaluate` & `rouge_score`: For measuring the quality of our summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q evaluate torch tqdm datasets peft transformers rouge_score\n",
    "! pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "We import the standard data science stack alongside the specific libraries for QLoRA (`BitsAndBytesConfig`, `LoraConfig`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "\n",
    "# Setup directories to keep our workspace clean\n",
    "DATASETS_DIR = \"./datasets\"\n",
    "OUTPUTS_DIR = \"./outputs\"\n",
    "CONFIG_FILE_PATH = \"./config.yaml\"\n",
    "\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The QLoRA Setup Function\n",
    "\n",
    "This is the heart of the optimization. Instead of loading the full model (which might require 16GB+ VRAM for a 7B model), we load it in **4-bit precision**.\n",
    "\n",
    "### Key Concepts:\n",
    "1.  **`load_in_4bit=True`**: Compresses weights to 4-bit.\n",
    "2.  **`bnb_4bit_quant_type=\"nf4\"`**: Uses \"NormalFloat4\", a data type optimized for the bell-curve distribution of neural network weights.\n",
    "3.  **`bnb_4bit_use_double_quant=True`**: Quantizes the quantization constants themselves to save even more memory.\n",
    "4.  **`bnb_4bit_compute_dtype`**: We store weights in 4-bit, but perform calculations in `bfloat16` for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str = CONFIG_FILE_PATH):\n",
    "    \"\"\"Helper to load the yaml config file.\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        # Fallback config if file doesn't exist (for tutorial purposes)\n",
    "        return {\n",
    "            \"base_model\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            \"dataset\": {\"name\": \"knkarthick/samsum\", \"splits\": {\"train\": 1000, \"validation\": 100, \"test\": 100}},\n",
    "            \"task_instruction\": \"Summarize the following dialogue.\",\n",
    "            \"sequence_len\": 512,\n",
    "            \"lora_r\": 16,\n",
    "            \"lora_alpha\": 32,\n",
    "            \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "            \"num_epochs\": 1,\n",
    "            \"batch_size\": 4,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"load_in_4bit\": True\n",
    "        }\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def setup_model_and_tokenizer(cfg, use_4bit: bool = None, use_lora: bool = None):\n",
    "    \"\"\"\n",
    "    Sets up the model with Quantization (BitsAndBytes) and LoRA adapters (PEFT).\n",
    "    \"\"\"\n",
    "    model_name = cfg[\"base_model\"]\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "\n",
    "    # 1. Setup Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Llama models often lack a pad token, so we use the EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" # Right padding is standard for training\n",
    "\n",
    "    # Check configuration overrides\n",
    "    load_in_4bit = use_4bit if use_4bit is not None else cfg.get(\"load_in_4bit\", False)\n",
    "    apply_lora = use_lora if use_lora is not None else (\"lora_r\" in cfg)\n",
    "\n",
    "    # 2. Configure Quantization (BitsAndBytes)\n",
    "    quant_cfg = None\n",
    "    if load_in_4bit:\n",
    "        print(\"‚öôÔ∏è  Enabling 4-bit quantization (NF4 + Double Quantization)...\")\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=cfg.get(\"bnb_4bit_quant_type\", \"nf4\"), # Normalized Float 4\n",
    "            bnb_4bit_use_double_quant=cfg.get(\"bnb_4bit_use_double_quant\", True),\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 # Compute in bf16 for speed/stability\n",
    "        )\n",
    "\n",
    "    # 3. Load Base Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=\"auto\",\n",
    "        # If not using quantization, load in bf16 directly\n",
    "        dtype=(torch.bfloat16 if cfg.get(\"bf16\", True) and torch.cuda.is_available() else torch.float32),\n",
    "    )\n",
    "\n",
    "    # 4. Apply LoRA Adapters\n",
    "    if apply_lora:\n",
    "        print(\"üîß Applying LoRA configuration...\")\n",
    "        # Prepares model for k-bit training (e.g. freezes weights, casts layer norms)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        lora_cfg = LoraConfig(\n",
    "            r=cfg.get(\"lora_r\", 8),               # Rank: The \"capacity\" of the adapter\n",
    "            lora_alpha=cfg.get(\"lora_alpha\", 16), # Alpha: Scaling factor (usually 2x rank)\n",
    "            target_modules=cfg.get(\"target_modules\", [\"q_proj\", \"v_proj\"]), # Target attention layers\n",
    "            lora_dropout=cfg.get(\"lora_dropout\", 0.05),\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "        \n",
    "        # Print how many parameters we are actually training (usually < 1%)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Helper Functions\n",
    "\n",
    "These helper functions allow us to easily load the SAMSum dataset and select specific subsets (e.g., just 1000 examples) to keep our training fast for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(dataset, n_samples, seed=42):\n",
    "    \"\"\"Helper to grab a random subset of data for quick iteration.\"\"\"\n",
    "    if n_samples == \"all\" or n_samples is None:\n",
    "        return dataset\n",
    "    if n_samples > len(dataset):\n",
    "        return dataset\n",
    "    return dataset.shuffle(seed=seed).select(range(n_samples))\n",
    "\n",
    "def load_and_prepare_dataset(cfg):\n",
    "    \"\"\"Loads the dataset from Hugging Face or local cache.\"\"\"\n",
    "    # 1. Parse config\n",
    "    if \"dataset\" in cfg:\n",
    "        cfg_dataset = cfg[\"dataset\"]\n",
    "        dataset_name = cfg_dataset[\"name\"]\n",
    "        n_train = cfg_dataset.get(\"splits\", {}).get(\"train\", \"all\")\n",
    "        n_val = cfg_dataset.get(\"splits\", {}).get(\"validation\", \"all\")\n",
    "    else:\n",
    "        # Fallback\n",
    "        dataset_name = \"knkarthick/samsum\"\n",
    "        n_train = 1000\n",
    "        n_val = 100\n",
    "\n",
    "    # 2. Download/Load\n",
    "    print(f\"‚¨áÔ∏è  Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # 3. Select Subsets\n",
    "    val_key = \"validation\" if \"validation\" in dataset else \"val\"\n",
    "    train = select_subset(dataset[\"train\"], n_train)\n",
    "    val = select_subset(dataset[val_key], n_val)\n",
    "    \n",
    "    print(f\"üìä Ready for training with {len(train)} train and {len(val)} validation samples.\")\n",
    "    return train, val, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing: Assistant-Only Masking\n",
    "\n",
    "This is **the most critical part** of instruction fine-tuning.\n",
    "\n",
    "When training on a conversation like:\n",
    "> **User:** Summarize this.\\n **Assistant:** Here is the summary.\n",
    "\n",
    "We do **NOT** want the model to learn how to write the user's prompt. We only want it to learn the assistant's response.\n",
    "\n",
    "### How we do it:\n",
    "1.  **Tokenize** the full conversation.\n",
    "2.  **Create Labels**: A copy of the input IDs.\n",
    "3.  **Masking**: We set the label ID to `-100` for all tokens belonging to the **User Prompt**.\n",
    "4.  **PyTorch behavior**: The CrossEntropyLoss function in PyTorch automatically ignores any index set to `-100`. Therefore, loss is only calculated on the Assistant's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(dialogue: str, task_instruction: str) -> str:\n",
    "    \"\"\"Formats the input into a standard prompt.\"\"\"\n",
    "    return f\"{task_instruction}\\n\\n## Dialogue:\\n{dialogue}\\n## Summary:\"\n",
    "\n",
    "def build_messages_for_sample(sample, task_instruction, include_assistant=False):\n",
    "    \"\"\"Creates the list of messages dictionary required by chat templates.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": build_user_prompt(sample[\"dialogue\"], task_instruction),\n",
    "        }\n",
    "    ]\n",
    "    if include_assistant:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": sample[\"summary\"]})\n",
    "    return messages\n",
    "\n",
    "def preprocess_samples(examples, tokenizer, task_instruction, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes data and applies Assistant-Only Masking.\n",
    "    \"\"\"\n",
    "    input_ids_list, labels_list, attn_masks = [], [], []\n",
    "\n",
    "    for d, s in zip(examples[\"dialogue\"], examples[\"summary\"]):\n",
    "        sample = {\"dialogue\": d, \"summary\": s}\n",
    "\n",
    "        # 1. Create the full conversation (User + Assistant)\n",
    "        msgs_full = build_messages_for_sample(sample, task_instruction, include_assistant=True)\n",
    "        \n",
    "        # 2. Create just the prompt (User only) to measure its length\n",
    "        msgs_prompt = build_messages_for_sample(sample, task_instruction, include_assistant=False)\n",
    "\n",
    "        # 3. Apply Chat Template (converts list of dicts to string)\n",
    "        text_full = tokenizer.apply_chat_template(msgs_full, tokenize=False)\n",
    "        text_prompt = tokenizer.apply_chat_template(msgs_prompt, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        prompt_len = len(text_prompt)\n",
    "\n",
    "        # 4. Tokenize the full text\n",
    "        tokens = tokenizer(\n",
    "            text_full,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True, # We need offsets to find where the prompt ends\n",
    "        )\n",
    "\n",
    "        # 5. Create Masking (The Magic Step)\n",
    "        # Find the token index where the prompt ends\n",
    "        start_idx = len(tokens[\"input_ids\"])\n",
    "        for i, (start, _) in enumerate(tokens[\"offset_mapping\"]):\n",
    "            if start >= prompt_len:\n",
    "                start_idx = i\n",
    "                break\n",
    "        \n",
    "        # Create labels: Mask the prompt part with -100\n",
    "        labels = [-100] * start_idx + tokens[\"input_ids\"][start_idx:]\n",
    "        \n",
    "        input_ids_list.append(tokens[\"input_ids\"])\n",
    "        labels_list.append(labels)\n",
    "        attn_masks.append(tokens[\"attention_mask\"])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"labels\": labels_list,\n",
    "        \"attention_mask\": attn_masks,\n",
    "    }\n",
    "\n",
    "def tokenize_dataset(cfg, tokenizer, train_data, val_data):\n",
    "    \"\"\"Applies the preprocessing to the whole dataset.\"\"\"\n",
    "    print(\"\\nTokenizing datasets...\")\n",
    "    fn = lambda e: preprocess_samples(e, tokenizer, cfg[\"task_instruction\"], cfg[\"sequence_len\"])\n",
    "    \n",
    "    tokenized_train = train_data.map(fn, batched=True, remove_columns=train_data.column_names)\n",
    "    tokenized_val = val_data.map(fn, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "    return tokenized_train, tokenized_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Collator\n",
    "\n",
    "Since our sequences are of different lengths, we need a custom collator to pad them dynamically per batch. \n",
    "Crucially, we must pad the `labels` with `-100` so that the padding tokens are ignored during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingCollator:\n",
    "    def __init__(self, tokenizer, label_pad_token_id=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Convert lists to tensors\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in batch]\n",
    "        attn_masks = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in batch]\n",
    "        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in batch]\n",
    "\n",
    "        # Pad to the max length in this batch\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attn_masks = pad_sequence(attn_masks, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # Important: Pad labels with -100 so loss ignores padding\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=self.label_pad_token_id)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_masks,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialization & Preprocessing Execution\n",
    "\n",
    "Let's put it all together: Load config, initialize the model/tokenizer, and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Config\n",
    "cfg = load_config()\n",
    "\n",
    "# 2. Setup Model (QLoRA)\n",
    "model, tokenizer = setup_model_and_tokenizer(cfg, use_4bit=True, use_lora=True)\n",
    "\n",
    "# 3. Load Data\n",
    "train_data, val_data, _ = load_and_prepare_dataset(cfg)\n",
    "\n",
    "# 4. Tokenize & Mask Data\n",
    "tokenized_train, tokenized_val = tokenize_dataset(cfg, tokenizer, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Inspecting the Data\n",
    "\n",
    "It's good practice to check if our masking worked. In the labels below, you should see a long sequence of `-100` at the start (masking the prompt), followed by actual token IDs (the response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "input_ids = tokenized_train[sample_idx]['input_ids']\n",
    "labels = tokenized_train[sample_idx]['labels']\n",
    "\n",
    "print(f\"Original Length: {len(input_ids)}\")\n",
    "print(f\"Label Length: {len(labels)}\")\n",
    "print(\"\\nFirst 20 labels (Should be mostly -100):\", labels[:20])\n",
    "print(\"Last 20 labels (Should be real IDs):\", labels[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "We use the Hugging Face `Trainer` class. Note the QLoRA specific optimizations:\n",
    "* `paged_adamw_8bit`: An optimizer that saves memory by using 8-bit statistics and paging to CPU RAM if GPU VRAM gets full.\n",
    "* `gradient_accumulation_steps`: Simulates a larger batch size without using more memory.\n",
    "* `fp16`/`bf16`: Mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg, model, tokenizer, tokenized_train, tokenized_val):\n",
    "    collator = PaddingCollator(tokenizer=tokenizer)\n",
    "\n",
    "    output_dir = os.path.join(OUTPUTS_DIR, \"lora_samsum\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=cfg[\"num_epochs\"],\n",
    "        per_device_train_batch_size=cfg[\"batch_size\"],\n",
    "        gradient_accumulation_steps=cfg[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=float(cfg[\"learning_rate\"]),\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=10,\n",
    "        bf16=True, # Use BF16 if A100/T4, otherwise FP16\n",
    "        optim=\"paged_adamw_8bit\", # Saves memory!\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\nStarting LoRA fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    # Save the adapters\n",
    "    model.save_pretrained(output_dir)\n",
    "    return model\n",
    "\n",
    "# Execute Training\n",
    "model = train_model(cfg, model, tokenizer, tokenized_train, tokenized_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion & Next Steps\n",
    "\n",
    "Congratulations! You have successfully fine-tuned a Large Language Model using QLoRA. \n",
    "\n",
    "### What happened?\n",
    "1.  We froze the massive base model parameters.\n",
    "2.  We trained small adapter matrices (LoRA) on top of them.\n",
    "3.  We masked the user prompts so the model only learned to generate summaries.\n",
    "\n",
    "### Optional: Push to Hub\n",
    "You can now push your adapters to the Hugging Face Hub to share them or load them later for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(model, tokenizer, model_name, hf_username):\n",
    "    model_id = f\"{hf_username}/{model_name}\"\n",
    "    print(f\"Pushing to {model_id}...\")\n",
    "    model.push_to_hub(model_id)\n",
    "    tokenizer.push_to_hub(model_id)\n",
    "\n",
    "# Uncomment to push\n",
    "# from google.colab import userdata\n",
    "# HF_USERNAME = userdata.get('HF_USERNAME')\n",
    "# push_to_hub(model, tokenizer, \"Llama-3.2-1B-QLoRA-Summarizer\", HF_USERNAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}